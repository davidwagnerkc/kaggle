{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "c259ab7c3cfb3b038626a45f263de859f192b1c8"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.dummy import Pool\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl_params = {\n",
    "    'figure.figsize': (10, 5),\n",
    "    'figure.dpi': 300,\n",
    "}\n",
    "from matplotlib import pyplot as plt\n",
    "mpl.rcParams.update(mpl_params)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "92a02ee04a7b992a07de1dfb25000124111a0308"
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = Path('../input/human-protein-atlas-image-classification/')\n",
    "# TRAIN_DIR = DATA_DIR / 'train'\n",
    "# TEST_DIR = DATA_DIR / 'test'\n",
    "\n",
    "# train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "# test_df = pd.read_csv(DATA_DIR / 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROCESSED = Path('test_processed')\n",
    "TEST_PROCESSED.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1a638cedbd14ff214fcdc95e99ae026a5b73128c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e331013f414d58d12f9aed0f8b338e6ea41fc534"
   },
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    0: 'Nucleoplasm', \n",
    "    1: 'Nuclear membrane',   \n",
    "    2: 'Nucleoli',   \n",
    "    3: 'Nucleoli fibrillar center' ,  \n",
    "    4: 'Nuclear speckles',\n",
    "    5: 'Nuclear bodies',\n",
    "    6: 'Endoplasmic reticulum',   \n",
    "    7: 'Golgi apparatus',\n",
    "    8: 'Peroxisomes',\n",
    "    9: 'Endosomes',\n",
    "    10: 'Lysosomes',\n",
    "    11: 'Intermediate filaments',   \n",
    "    12: 'Actin filaments',\n",
    "    13: 'Focal adhesion sites',   \n",
    "    14: 'Microtubules',\n",
    "    15: 'Microtubule ends',   \n",
    "    16: 'Cytokinetic bridge',   \n",
    "    17: 'Mitotic spindle',\n",
    "    18: 'Microtubule organizing center',  \n",
    "    19: 'Centrosome',\n",
    "    20: 'Lipid droplets',   \n",
    "    21: 'Plasma membrane',   \n",
    "    22: 'Cell junctions', \n",
    "    23: 'Mitochondria',\n",
    "    24: 'Aggresome',\n",
    "    25: 'Cytosol',\n",
    "    26: 'Cytoplasmic bodies',   \n",
    "    27: 'Rods & rings'\n",
    "}\n",
    "\n",
    "LABEL_NAMES = list(LABELS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "6c8d69d944c8c8a51a5f246e0f5413af149960f3"
   },
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, df, images_dir, transform=None):            \n",
    "        self.df = df.copy()\n",
    "        self._dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.p = Pool(1)\n",
    "        self.mlb = MultiLabelBinarizer(list(range(len(LABELS))))\n",
    "        self.count = 0\n",
    "        self.total_load = 0\n",
    "        self.total_stack = 0\n",
    "        self.total_transform = 0\n",
    "        self.colors = ['red', 'green', 'blue', 'yellow']\n",
    "        self.cache_size = len(self.df)\n",
    "        self.latest = 0\n",
    "        self.stack = []\n",
    "        self.save = iter(list(range(32)))\n",
    "        \n",
    "        self.cache = {}\n",
    "#         for i in range(self.cache_size):\n",
    "#             self.latest = i\n",
    "#             id_ = self.df.iloc[i].Id\n",
    "#             image_paths = [self._dir / f'{id_}_{c}.png' for c in self.colors]\n",
    "#             self.cache[i] = self.p.map_async(self.mp_load, image_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def mp_load(self, path):\n",
    "        pil_im = Image.open(path)\n",
    "        return np.array(pil_im, np.uint8)\n",
    "                                      \n",
    "    def __getitem__(self, key):\n",
    "        self.count += 1\n",
    "        id_ = self.df.iloc[key].Id\n",
    "        \n",
    "        image_paths = [self._dir / f'{id_}_{c}.png' for c in self.colors]\n",
    "        t1 = time.time()\n",
    "        if key in self.cache:\n",
    "            r, g, b, y = self.cache.pop(key).get()\n",
    "        else:\n",
    "            r, g, b, y = self.p.map(self.mp_load, image_paths)\n",
    "        self.total_load += time.time() - t1\n",
    "        \n",
    "        t1 = time.time()\n",
    "        rgb = np.stack([\n",
    "            r // 2 + y // 2,\n",
    "            g // 2 + y // 2,\n",
    "            b // 2\n",
    "        ], axis=2)\n",
    "        self.total_stack += time.time() - t1\n",
    "        \n",
    "        y = []\n",
    "        if 'Target' in self.df:\n",
    "            y = list(map(int, self.df.iloc[key].Target.split(' ')))\n",
    "            y = self.mlb.fit_transform([y]).squeeze()\n",
    "            \n",
    "        if transform:\n",
    "            t1 = time.time()\n",
    "            X = transform(rgb)\n",
    "            self.total_transform += time.time() - t1\n",
    "        else:\n",
    "            X = rgb\n",
    "            \n",
    "        self.stack.append(np.array(X))\n",
    "        \n",
    "        if len(self.stack) == (len(self.df) / 2):\n",
    "            np.savez_compressed(TEST_PROCESSED / f'{next(self.save)}-processed.npz', *self.stack)\n",
    "            del self.stack\n",
    "            self.stack = []\n",
    "            \n",
    "        fn = f'{id_}.png'\n",
    "        return None #(np.array(X), y, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9207698551ecaad9231efc9149c51b50f4336e6a"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((299, 299)),  # (299, 299) InceptionV3 input\n",
    "    transforms.ToTensor(),  # To Tensor dtype and convert [0, 255] uint8 to [0, 1] float\n",
    "    transforms.Normalize(  # Standard image normalization\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8b1695b7ecfef947753a6638b1f3dca220343b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 ms, sys: 1.4 ms, total: 3.56 ms\n",
      "Wall time: 2.38 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_ds = ProteinDataset(\n",
    "    df=train_df,\n",
    "    images_dir=TRAIN_DIR,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e85d78c8bbea1c6c443135dd65a83ad8fc53779a"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "10.592767000198364\n",
      "1024\n",
      "21.134756803512573\n",
      "1536\n",
      "31.707923889160156\n",
      "2048\n",
      "42.11814570426941\n",
      "2560\n",
      "52.578545808792114\n",
      "3072\n",
      "62.97861194610596\n",
      "3584\n",
      "73.34701895713806\n",
      "4096\n",
      "83.82030177116394\n",
      "4608\n",
      "94.37246584892273\n",
      "5120\n",
      "104.96391201019287\n",
      "5632\n",
      "115.53500986099243\n",
      "6144\n",
      "295.10383892059326\n",
      "6656\n",
      "306.67052388191223\n",
      "7168\n",
      "317.6403658390045\n",
      "7680\n",
      "328.4414279460907\n",
      "8192\n",
      "340.0331027507782\n",
      "8704\n",
      "350.800724029541\n",
      "9216\n",
      "361.57620787620544\n",
      "9728\n",
      "372.99183082580566\n",
      "10240\n",
      "384.1622977256775\n",
      "10752\n",
      "395.59630393981934\n",
      "11264\n",
      "406.91136479377747\n"
     ]
    }
   ],
   "source": [
    "# save_pool = Pool(8)\n",
    "count = 0\n",
    "t1 = time.time()\n",
    "for _ in test_ds:\n",
    "    count += 1\n",
    "#     save_pool.apply_async(np.save, args=(PROCESSED / z.replace('png', 'npy'), X))\n",
    "    if count % 512 == 0:\n",
    "        print(count)\n",
    "        print(time.time() - t1)\n",
    "# np.savez_compressed(PROCESSED / 'processed.npz', *train_ds.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0cd35f3e2ed3f36503a4eeb3565ee2fc7807f988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.4 s, sys: 235 ms, total: 31.7 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.savez_compressed(PROCESSED / 'proc.npz', *train_ds.stack[:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31072"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31072"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(train_ds.stack):\n",
    "    torch.save(t, f'{i}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "477209542cb9c92cb64e62f42539cc66a80cf392"
   },
   "outputs": [],
   "source": [
    "# '{total_load:.4f}, {total_stack:.4f}, {total_transform:.4f}'.format(**train_ds.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57e17d6d06824561ec9287cbeb218241269a58c8"
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.inception_v3(pretrained=True, transform_input=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "928803326425b7cc17b7dd14ca65ab8b730221e3"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "523c11545fc89f93b7f055b47abb91400d031d6d"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c352e8d8e7039ddfbc50bfd7921b0c77eaf0a21"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name.startswith('Mixed_7') or name.startswith('Mixed_6'):\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, len(LABELS))\n",
    "# torch.nn.init.xavier_uniform_(model.fc.weight)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = FocalLoss()\n",
    "# criterion = nn.MultiLabelMarginLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad),\n",
    "    lr=0.001,\n",
    ")\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "944bb3c6c3dae9ce96e52407c166ac6ee0fcf301"
   },
   "outputs": [],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    ts_start = time.time()\n",
    "    print(f'Epoch {epoch+1}/{N_EPOCHS}')\n",
    "    model.train()\n",
    "    \n",
    "    running_loss, correct, count= 0.0, 0, 0\n",
    "    for X, y in train_dl:\n",
    "        count += BATCH_SIZE\n",
    "        X, y = X.to(device, dtype=torch.float), y.to(device, dtype=torch.float)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            y_, _ = model(X)\n",
    "            loss = criterion(y_, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if count % 128 == 0:\n",
    "            print(f'    batch loss: {loss.item():0.3f}')\n",
    "            print(f'    epoch time: {time.time() - ts_start:0.3f}')\n",
    "        \n",
    "#         y_label_ = (sigmoid(y_) > .5).float()\n",
    "#         correct += (y_label_ == y).sum().item()\n",
    "#         running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "#     print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "#     print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "\n",
    "#     # Eval\n",
    "#     model.eval()  # IMPORTANT\n",
    "    \n",
    "#     running_loss, correct = 0.0, 0.0\n",
    "#     with torch.no_grad():  # IMPORTANT\n",
    "#         for X, y in val_dl:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "                    \n",
    "#             y_ = model(X)\n",
    "        \n",
    "#             y_label_ = (y_ > .5).float()\n",
    "#             correct += (y_label_ == y).sum().item()\n",
    "            \n",
    "#             loss = criterion(y_, y.squeeze())\n",
    "#             running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "#     print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "#     print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b3096308c03767518272841634d4327b6202ca1"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "4c1bb6c5e45609610db021ffd05c7a1e4523e1d4"
   },
   "outputs": [],
   "source": [
    "test_ds = ProteinDataset(\n",
    "    df=test_df,\n",
    "    images_dir=TEST_DIR,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c36b7ac52d08ccb17ac5348695601f349238f6dd"
   },
   "outputs": [],
   "source": [
    "SUBMISSION_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeb8f5a0f201516f3336448f955f87e9904f0c3b"
   },
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d37288e3f32733b62f056c0528fd66cc1ca2d76"
   },
   "outputs": [],
   "source": [
    "# Eval\n",
    "model.eval()\n",
    "\n",
    "# y_predict = []\n",
    "ys = []\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    for X, _ in test_dl:\n",
    "        X = X.to(device, dtype=torch.float)\n",
    "\n",
    "        y_ = model(X)\n",
    "        y_ = sigmoid(y_)\n",
    "        y_ = y_.to('cpu').numpy()\n",
    "\n",
    "#         y_label_ = np.array(sigmoid(y_) > .5, dtype=np.float)\n",
    "        \n",
    "#         y_predict.extend(y_label_)\n",
    "        ys.extend(y_)\n",
    "        print(time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aca359a73b09e4ace231c8c130adbf4dc687e1f9"
   },
   "outputs": [],
   "source": [
    "ys_stack = np.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "182012294d3e3f7ea7984c663ecd4bd08b02fdc0"
   },
   "outputs": [],
   "source": [
    "if SUBMISSION_RUN:\n",
    "    submission = test_df.copy()\n",
    "    Predicted = []\n",
    "    for i, prediction in enumerate(train_ds.mlb.inverse_transform(ys_stack > .4)):\n",
    "        if len(prediction) == 0:\n",
    "            prediction = tuple([np.argmax(ys_stack[i])])\n",
    "        all_labels = []\n",
    "        for label in prediction:\n",
    "            all_labels.append(str(label))\n",
    "        Predicted.append(' '.join(all_labels))\n",
    "\n",
    "    submission['Predicted'] = Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3feae9968e861f12a9b90590e72877487a291143"
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ca1327ec9ffdc31255cdd7bb71068883926f5627"
   },
   "outputs": [],
   "source": [
    "if SUBMISSION_RUN:\n",
    "    submission.to_csv('protein_classification.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2af10a2117ede0400867562872ade9140f4959c8"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f99914929a87cfef1f1a7177d45335fe8b22f17"
   },
   "outputs": [],
   "source": [
    "np.save('ys_stack.npy', ys_stack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
